{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DisguiseFaceRecognitionViaTransferLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMdSDoGw2zPLghHJZW8Xfc7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tommyj530/DisguiseFaceRecognition/blob/main/DisguiseFaceRecognitionViaTransferLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYwrZZPrdHzd",
        "outputId": "f11b0b9f-d426-462e-a308-d8aa8cb5baa9"
      },
      "source": [
        "!pip install livelossplot --quiet\n",
        "!pip install split-folders\n",
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import zipfile\n",
        "from livelossplot import PlotLosses\n",
        "import splitfolders\n",
        "\n",
        "plt.ion()   # interactive mode"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting split-folders\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/5f/3c2b2f7ea5e047c8cdc3bb00ae582c5438fcdbbedcc23b3cc1c2c7aae642/split_folders-0.4.3-py3-none-any.whl\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.4.3\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etOv3Xr-dYCQ"
      },
      "source": [
        "Data Organization\n",
        "\n",
        "Disguise vs Original\n",
        "Train and Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yU4b_4YudZV8"
      },
      "source": [
        "from zipfile import ZipFile \n",
        "file_name = \"DisguiseFaceDataset.zip\"\n",
        "\n",
        "with ZipFile(file_name,'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Done')\n",
        "\n",
        "# Split the data into \"train\" and \"validation\"\n",
        "#splitfolders.ratio('DisguiseFaceDataset', output=\"TrainTestDataset\", seed=1337, ratio=(.8, .2), group_prefix=None) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2R0EFKrndmwn"
      },
      "source": [
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "\n",
        "'''\n",
        "For the project, we will use different transformations for the model \n",
        "to learn invariant features from the training dataset \n",
        "a) rotation/flip\n",
        "b) de-exturized \n",
        "c) de-colorized \n",
        "d) edged enhanced \n",
        "e) salient edge map\n",
        "'''\n",
        "\n",
        "#increased_dataset = torch.utils.data.ConcatDataset([transformed_dataset,original])\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        #transforms.ColorJitter(),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "        #transforms.RandomRotation(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "data_dir = \"TestTrainDataset\"\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
        "                                             shuffle=True, num_workers=4)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "# Splitting data into train and validation \n",
        "'''\n",
        "X_train, X_test, Y_train, Y_test = generate_train_test_pairs(1000)\n",
        "\n",
        "trainloader = DataLoader(TensorDataset(torch.from_numpy(X_train), torch.from_numpy(Y_train)),\n",
        "                         batch_size=32, shuffle=True)\n",
        "testloader = DataLoader(TensorDataset(torch.from_numpy(X_test), torch.from_numpy(Y_test)),\n",
        "                         batch_size=32, shuffle=False)\n",
        "\n",
        "dataloaders = {\n",
        "    \"train\": trainloader,\n",
        "    \"validation\": testloader\n",
        "}\n",
        "'''\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y-WIhludr_-"
      },
      "source": [
        "Visualizing images "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAWbWph4dwAx"
      },
      "source": [
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(dataloaders['train']))\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "imshow(out, title=[class_names[x] for x in classes])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76B9sNyddyFv"
      },
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70N57mJGeW7y"
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    liveloss = PlotLosses()\n",
        "    tra_acc_history = []\n",
        "    val_acc_history = []\n",
        "\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        logs = {}\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            \n",
        "            # Record training accuracy over epoch\n",
        "            if phase == 'train':\n",
        "                tra_acc_history.append(epoch_acc)\n",
        " \n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "            prefix = ''   \n",
        "            if phase == 'val':    \n",
        "                prefix = 'val_'\n",
        "                val_acc_history.append(epoch_acc)\n",
        "\n",
        "            logs[prefix + 'log loss'] = epoch_loss\n",
        "            logs[prefix + 'accuracy'] = epoch_acc\n",
        "        print()\n",
        "        liveloss.update(logs)\n",
        "        liveloss.send()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCZNmZaXeaij"
      },
      "source": [
        "Visualizing the model predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqoqZcovegIY"
      },
      "source": [
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                ax.set_ylabel('true: {}'.format(class_names[labels[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sdryVj4ehWx"
      },
      "source": [
        "Finetuning the DCNN (pre-trained) : Resnet18"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAHpr9xDel7W"
      },
      "source": [
        "model_ft = models.resnet18(pretrained=True)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "print(num_ftrs)\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names))\n",
        "'''\n",
        "self.labels_model = nn.Sequential(\n",
        "            nn.Linear(fc_inputs, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(256, num_classes), \n",
        "            nn.LogSoftmax(dim=1) # For using NLLLoss()\n",
        ")\n",
        "'''\n",
        "\n",
        "model_ft.fc = nn.Sequential(\n",
        "          nn.Dropout(0.4),\n",
        "          nn.Linear(num_ftrs, 2)    \n",
        ")\n",
        "\n",
        "model_ft.fc = nn.Linear(num_ftrs, 2)    \n",
        "\n",
        "# Loading the parameters from previous training HERE \n",
        "#model_ft.load_state_dict(state_dict)\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIv_Dbb1eopW"
      },
      "source": [
        "Train and evaluate\n",
        "\n",
        "The model seems to be overfitting by quite a bit. No need to conduct more than 30 epochs as the validation accuracy seems to reach its asymptotic limit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No5-xSZ_epoY"
      },
      "source": [
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O2CW2MDetVE"
      },
      "source": [
        "Freezing all the network except the final layer (Need to set requires_grad == False)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clAqQ6SSe19y"
      },
      "source": [
        "model_conv = torchvision.models.resnet18(pretrained=True)\n",
        "for param in model_conv.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "num_ftrs = model_conv.fc.in_features\n",
        "\n",
        "model_conv.fc = nn.Sequential(\n",
        "          nn.Dropout(0.5),\n",
        "          nn.Linear(num_ftrs, 2)  \n",
        ")\n",
        "        \n",
        "model_conv = model_conv.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that only parameters of final layer are being optimized as\n",
        "# opposed to before.\n",
        "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arvO9mwRe306"
      },
      "source": [
        "Train and evaluate ONLY the FINAL layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUdjclNee4F2"
      },
      "source": [
        "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
        "                         exp_lr_scheduler, num_epochs=25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQM10YCXfBAC"
      },
      "source": [
        "Saving hyperparameters for later use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3duoP5kfFTr"
      },
      "source": [
        "torch.save(model_ft.state_dict(), '30EpochsWithDropout.pth')\n",
        "\n",
        "# download checkpoint file\n",
        "files.download('30EpochsWithDropout.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuWQfr2ifJLo"
      },
      "source": [
        "Visualize the model prediction with true values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cefhynP5fHO3"
      },
      "source": [
        "visualize_model(model_ft)\n",
        "\n",
        "plt.ioff()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}