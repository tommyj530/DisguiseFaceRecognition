{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import cv2\n",
    "import numpy as np \n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics.pairwise import chi2_kernel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import h5py\n",
    "import glob\n",
    "import warnings\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/18rvb/ELEC872/DisguiseFaceRecognition/TrainTestDataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Features (Color Histograms, Haralick Textures, Hu Moments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature-descriptor-1: Hu Moments\n",
    "bins = 8\n",
    "\n",
    "def fd_hu_moments(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    feature = cv2.HuMoments(cv2.moments(image)).flatten()\n",
    "    return feature\n",
    "\n",
    "# # feature-descriptor-2: Haralick Texture\n",
    "# def fd_haralick(image):\n",
    "#     # convert the image to grayscale\n",
    "#     gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#     # compute the haralick texture feature vector\n",
    "#     haralick = mahotas.features.haralick(gray).mean(axis=0)\n",
    "#     # return the result\n",
    "#     return haralick\n",
    "\n",
    "# feature-descriptor-3: Color Histogram\n",
    "def fd_histogram(image, mask=None):\n",
    "    # convert the image to HSV color-space\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    # compute the color histogram\n",
    "    hist  = cv2.calcHist([image], [0, 1, 2], None, [bins, bins, bins], [0, 256, 0, 256, 0, 256])\n",
    "    # normalize the histogram\n",
    "    cv2.normalize(hist, hist)\n",
    "    # return the histogram\n",
    "    return hist.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to read images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_imgs(img_path, fixed_size=tuple((500, 500))):\n",
    "    image = pyplot.imread(img_path)\n",
    "    image = cv2.resize(image, fixed_size)\n",
    "    image_new = np.array(image, dtype=np.uint8)\n",
    "#     image_new = image.astype('uint8')\n",
    "    gray_img = cv2.cvtColor(image_new, cv2.COLOR_BGR2GRAY)\n",
    "    return image_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load images from training directory and create labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = data_dir + '/train'\n",
    "\n",
    "disguised_paths = []\n",
    "original_paths = []\n",
    "disguised_labels = []\n",
    "original_labels = []\n",
    "\n",
    "import os.path\n",
    "for path, directories, files in os.walk(train_path + '/Disguise'):\n",
    "        for file in files:\n",
    "            disguised_paths.append(os.path.join(path, file))\n",
    "            disguised_labels.append(0)\n",
    "            \n",
    "for path, directories, files in os.walk(train_path + '/Original'):\n",
    "        for file in files:\n",
    "            original_paths.append(os.path.join(path, file))\n",
    "            original_labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = data_dir + '/val'\n",
    "\n",
    "test_disguised_paths = []\n",
    "test_original_paths = []\n",
    "test_disguised_labels = []\n",
    "test_original_labels = []\n",
    "\n",
    "import os.path\n",
    "for path, directories, files in os.walk(test_path + '/Disguise'):\n",
    "        for file in files:\n",
    "            test_disguised_paths.append(os.path.join(path, file))\n",
    "            test_disguised_labels.append(0)\n",
    "            \n",
    "for path, directories, files in os.walk(test_path + '/Original'):\n",
    "        for file in files:\n",
    "            test_original_paths.append(os.path.join(path, file))\n",
    "            test_original_labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "img_paths = []\n",
    "\n",
    "labels.extend(disguised_labels)\n",
    "labels.extend(original_labels)\n",
    "labels.extend(test_disguised_labels)\n",
    "labels.extend(test_original_labels)\n",
    "\n",
    "img_paths.extend(disguised_paths)\n",
    "img_paths.extend(original_paths)\n",
    "img_paths.extend(test_disguised_paths)\n",
    "img_paths.extend(test_original_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4343,)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(labels).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATUS] feature vector size (3945, 519)\n"
     ]
    }
   ],
   "source": [
    "global_features = []\n",
    "# errored_imgs = []\n",
    "labels_final = []\n",
    "\n",
    "for (num, x), lbls in zip(enumerate(img_paths), labels):\n",
    "#     try:\n",
    "    if num not in errored_imgs:\n",
    "        image = read_imgs(x)\n",
    "        fv_hu_moments = fd_hu_moments(image)\n",
    "        fv_histogram  = fd_histogram(image)\n",
    "        labels_final.append(lbls)\n",
    "#     except:\n",
    "#         errored_imgs.append(num)\n",
    "\n",
    "        global_feature = np.hstack([fv_histogram, fv_hu_moments])\n",
    "        global_features.append(global_feature)\n",
    "\n",
    "print(\"[STATUS] feature vector size {}\".format(np.array(global_features).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale features in the range (0-1)\n",
    "scaler            = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaled_features = scaler.fit_transform(global_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3945, 519)\n",
      "(3945,)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(rescaled_features).shape)\n",
    "print(np.array(labels_final).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trees = 100\n",
    "test_size = 0.10\n",
    "seed      = 9\n",
    "\n",
    "(trainDataGlobal, testDataGlobal, trainLabelsGlobal, testLabelsGlobal) = train_test_split(np.array(rescaled_features),\n",
    "                                                                                          np.array(labels_final),\n",
    "                                                                                          test_size=test_size,\n",
    "                                                                                          random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATUS] splitted train and test data...\n",
      "Train data  : (3550, 519)\n",
      "Test data   : (395, 519)\n",
      "Train labels: (3550,)\n",
      "Test labels : (395,)\n"
     ]
    }
   ],
   "source": [
    "print(\"[STATUS] splitted train and test data...\")\n",
    "print(\"Train data  : {}\".format(trainDataGlobal.shape))\n",
    "print(\"Test data   : {}\".format(testDataGlobal.shape))\n",
    "print(\"Train labels: {}\".format(trainLabelsGlobal.shape))\n",
    "print(\"Test labels : {}\".format(testLabelsGlobal.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('LR', LogisticRegression(random_state=seed)))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier(random_state=seed)))\n",
    "models.append(('RF', RandomForestClassifier(n_estimators=num_trees, random_state=seed)))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(random_state=seed)))\n",
    "\n",
    "scoring    = \"accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.695211 (0.018072)\n",
      "LDA: 0.682254 (0.018116)\n",
      "KNN: 0.686197 (0.024789)\n",
      "CART: 0.697746 (0.023162)\n",
      "RF: 0.737183 (0.018730)\n",
      "NB: 0.513803 (0.017465)\n",
      "SVM: 0.700845 (0.020461)\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "names   = []\n",
    "\n",
    "# 10-fold cross validation\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = cross_val_score(model, trainDataGlobal, trainLabelsGlobal, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
